Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 4)    112         input_1[0][0]                    
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 16, 16, 4)    0           conv2d[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 4)    0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 16, 16, 4)    0           leaky_re_lu[0][0]                
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_3[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_4[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_5[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_6[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 4)    0           leaky_re_lu_7[0][0]              
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 16, 16, 4)    16          dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 16, 16, 4)    16          dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 16, 16, 4)    16          dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 16, 16, 4)    16          dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 16, 16, 4)    16          dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 16, 16, 4)    16          dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 4)    16          dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 4)    16          dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 16, 16, 8)    0           batch_normalization[0][0]        
                                                                 batch_normalization_1[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 16, 16, 8)    0           batch_normalization_2[0][0]      
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 16, 16, 8)    0           batch_normalization_4[0][0]      
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 16, 16, 8)    0           batch_normalization_6[0][0]      
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 8, 8, 16)     1168        concatenate[0][0]                
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 8, 8, 16)     1168        concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 8, 8, 16)     1168        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 8, 8, 16)     1168        concatenate_3[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)       (None, 8, 8, 16)     0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 16)     0           conv2d_9[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 16)     0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 16)     0           conv2d_11[0][0]                  
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 8, 8, 16)     0           leaky_re_lu_8[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 16)     0           leaky_re_lu_9[0][0]              
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 16)     0           leaky_re_lu_10[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 16)     0           leaky_re_lu_11[0][0]             
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 8, 8, 16)     64          dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 8, 8, 16)     64          dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 8, 16)     64          dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 16)     64          dropout_11[0][0]                 
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 8, 8, 32)     0           batch_normalization_8[0][0]      
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 8, 8, 32)     0           batch_normalization_10[0][0]     
                                                                 batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 4, 4, 32)     9248        concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 4, 4, 32)     9248        concatenate_5[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)      (None, 4, 4, 32)     0           conv2d_12[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)      (None, 4, 4, 32)     0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 4, 4, 32)     0           leaky_re_lu_12[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 4, 4, 32)     0           leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 4, 4, 32)     128         dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 4, 4, 32)     128         dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 4, 4, 64)     0           batch_normalization_12[0][0]     
                                                                 batch_normalization_13[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 1024)         0           concatenate_6[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          262400      flatten[0][0]                    
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 256)          0           dense[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 256)          1024        dropout_14[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           2570        batch_normalization_14[0][0]     
==================================================================================================
Total params: 290,698
Trainable params: 289,866
Non-trainable params: 832
__________________________________________________________________________________________________
None
Epoch 1/100
1563/1563 [==============================] - 26s 17ms/step - loss: 2.0315 - accuracy: 0.3140 - val_loss: 2.0104 - val_accuracy: 0.3635
Epoch 2/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.4811 - accuracy: 0.4663 - val_loss: 1.2569 - val_accuracy: 0.5518
Epoch 3/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.3661 - accuracy: 0.5073 - val_loss: 1.2206 - val_accuracy: 0.5587
Epoch 4/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.2973 - accuracy: 0.5345 - val_loss: 1.2464 - val_accuracy: 0.5714
Epoch 5/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.2548 - accuracy: 0.5554 - val_loss: 1.2085 - val_accuracy: 0.5765
Epoch 6/100
1563/1563 [==============================] - 26s 16ms/step - loss: 1.2210 - accuracy: 0.5640 - val_loss: 1.1028 - val_accuracy: 0.6051
Epoch 7/100
1563/1563 [==============================] - 26s 17ms/step - loss: 1.1913 - accuracy: 0.5765 - val_loss: 1.1431 - val_accuracy: 0.5961
Epoch 8/100
1563/1563 [==============================] - 26s 16ms/step - loss: 1.1783 - accuracy: 0.5820 - val_loss: 1.1284 - val_accuracy: 0.5933
Epoch 9/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.1496 - accuracy: 0.5932 - val_loss: 1.1034 - val_accuracy: 0.6143
Epoch 10/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.1449 - accuracy: 0.5894 - val_loss: 1.1266 - val_accuracy: 0.5958
Epoch 11/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.1213 - accuracy: 0.5998 - val_loss: 1.4352 - val_accuracy: 0.5096
Epoch 12/100
1563/1563 [==============================] - 27s 18ms/step - loss: 1.0963 - accuracy: 0.6128 - val_loss: 1.0039 - val_accuracy: 0.6403
Epoch 13/100
1563/1563 [==============================] - 38s 25ms/step - loss: 1.0959 - accuracy: 0.6119 - val_loss: 0.9680 - val_accuracy: 0.6594
Epoch 14/100
1563/1563 [==============================] - 29s 18ms/step - loss: 1.0764 - accuracy: 0.6183 - val_loss: 1.0816 - val_accuracy: 0.6200
Epoch 15/100
1563/1563 [==============================] - 26s 17ms/step - loss: 1.0673 - accuracy: 0.6216 - val_loss: 0.9673 - val_accuracy: 0.6634
Epoch 16/100
1563/1563 [==============================] - 26s 17ms/step - loss: 1.0438 - accuracy: 0.6322 - val_loss: 0.9975 - val_accuracy: 0.6499
Epoch 17/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0460 - accuracy: 0.6258 - val_loss: 0.9150 - val_accuracy: 0.6710
Epoch 18/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0371 - accuracy: 0.6333 - val_loss: 1.0513 - val_accuracy: 0.6343
Epoch 19/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0239 - accuracy: 0.6404 - val_loss: 0.9147 - val_accuracy: 0.6763
Epoch 20/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0190 - accuracy: 0.6433 - val_loss: 0.9116 - val_accuracy: 0.6752
Epoch 21/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0053 - accuracy: 0.6465 - val_loss: 0.9146 - val_accuracy: 0.6808
Epoch 22/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0041 - accuracy: 0.6474 - val_loss: 1.0241 - val_accuracy: 0.6344
Epoch 23/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0005 - accuracy: 0.6453 - val_loss: 0.9450 - val_accuracy: 0.6618
Epoch 24/100
1563/1563 [==============================] - 25s 16ms/step - loss: 1.0000 - accuracy: 0.6473 - val_loss: 0.8819 - val_accuracy: 0.6866
Epoch 25/100
1563/1563 [==============================] - 26s 16ms/step - loss: 0.9854 - accuracy: 0.6516 - val_loss: 1.0825 - val_accuracy: 0.6148
Epoch 26/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9912 - accuracy: 0.6478 - val_loss: 0.9272 - val_accuracy: 0.6744
Epoch 27/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9747 - accuracy: 0.6547 - val_loss: 1.1005 - val_accuracy: 0.6146
Epoch 28/100
1563/1563 [==============================] - 28s 18ms/step - loss: 0.9833 - accuracy: 0.6513 - val_loss: 0.8812 - val_accuracy: 0.6941
Epoch 29/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9694 - accuracy: 0.6595 - val_loss: 0.9201 - val_accuracy: 0.6790
Epoch 30/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9642 - accuracy: 0.6607 - val_loss: 0.8989 - val_accuracy: 0.6803
Epoch 31/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9737 - accuracy: 0.6559 - val_loss: 0.8549 - val_accuracy: 0.7008
Epoch 32/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9645 - accuracy: 0.6611 - val_loss: 0.9167 - val_accuracy: 0.6741
Epoch 33/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9583 - accuracy: 0.6615 - val_loss: 0.9310 - val_accuracy: 0.6695
Epoch 34/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9424 - accuracy: 0.6672 - val_loss: 0.8547 - val_accuracy: 0.6994
Epoch 35/100
1563/1563 [==============================] - 26s 16ms/step - loss: 0.9477 - accuracy: 0.6692 - val_loss: 0.9033 - val_accuracy: 0.6777
Epoch 36/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9340 - accuracy: 0.6696 - val_loss: 0.9413 - val_accuracy: 0.6685
Epoch 37/100
1563/1563 [==============================] - 26s 16ms/step - loss: 0.9395 - accuracy: 0.6685 - val_loss: 0.8676 - val_accuracy: 0.6944
Epoch 38/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9439 - accuracy: 0.6677 - val_loss: 0.8671 - val_accuracy: 0.6943
Epoch 39/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9372 - accuracy: 0.6685 - val_loss: 0.8466 - val_accuracy: 0.6993
Epoch 40/100
1563/1563 [==============================] - 25s 16ms/step - loss: 0.9318 - accuracy: 0.6674 - val_loss: 0.8678 - val_accuracy: 0.6956
Epoch 41/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9327 - accuracy: 0.6726 - val_loss: 0.9817 - val_accuracy: 0.6539
Epoch 42/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9185 - accuracy: 0.6759 - val_loss: 0.8480 - val_accuracy: 0.6972
Epoch 43/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9146 - accuracy: 0.6764 - val_loss: 0.8338 - val_accuracy: 0.7075
Epoch 44/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9135 - accuracy: 0.6758 - val_loss: 0.8762 - val_accuracy: 0.6893
Epoch 45/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9246 - accuracy: 0.6731 - val_loss: 0.9141 - val_accuracy: 0.6852
Epoch 46/100
1563/1563 [==============================] - 27s 17ms/step - loss: 0.9193 - accuracy: 0.6742 - val_loss: 0.8621 - val_accuracy: 0.6935
Epoch 47/100
1563/1563 [==============================] - 26s 16ms/step - loss: 0.9086 - accuracy: 0.6793 - val_loss: 0.8233 - val_accuracy: 0.7105
Epoch 48/100
1563/1563 [==============================] - 26s 16ms/step - loss: 0.9055 - accuracy: 0.6824 - val_loss: 0.8733 - val_accuracy: 0.6861
Epoch 49/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9099 - accuracy: 0.6787 - val_loss: 0.8370 - val_accuracy: 0.7085
Epoch 50/100
1563/1563 [==============================] - 26s 17ms/step - loss: 0.9130 - accuracy: 0.6784 - val_loss: 0.8387 - val_accuracy: 0.7098
Epoch 51/100
 495/1563 [========>.....................] - ETA: 16s - loss: 0.8993 - accuracy: 0.6821